{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "molecular-china",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get meshes from https://github.com/gpeyre/2015-SIGGRAPH-convolutional-ot\n",
    "# install libigl: https://libigl.github.io/libigl-python-bindings/\n",
    "# install meshplot: https://anaconda.org/conda-forge/meshplot (https://skoch9.github.io/meshplot/tutorial/)\n",
    "# get meshes from https://github.com/libigl/libigl-tutorial-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "super-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_3d\n",
    "import meshplot as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchgeometry as tgm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import igl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "handmade-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_box(p):\n",
    "    v_box = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n",
    "                  [1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])\n",
    "    edges = np.array([\n",
    "        [0, 1], [0, 2], [1,3], [3, 2], [0, 4], [4, 5], [5, 1], [4, 6], [6, 7], [7, 5], [7, 3], [6, 2]\n",
    "    ])\n",
    "    p.add_points(v_box, shading={\"point_color\": \"green\", \"point_size\": 0.1})\n",
    "    p.add_edges(v_box, edges, shading={\"line_color\": \"green\"})\n",
    "\n",
    "def visualize(cube, p=None, color='red', point_size=0.03):\n",
    "    voxels = torch.stack(torch.where(cube[0])).T.numpy()\n",
    "    if p is None:\n",
    "        p = mp.plot(voxels/SIZE, c=color, shading={\"point_size\": point_size})\n",
    "    else:\n",
    "        p.add_points(voxels/SIZE, c=color, shading={\"point_size\": point_size})\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "blocked-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_offset_and_normalize_custom(path, off_type='off'):\n",
    "    if off_type=='off':\n",
    "        v_2, f_2, _ = igl.read_off(path)\n",
    "    elif off_type=='obj':\n",
    "        v_2, _, _, f_2, _, _ = igl.read_obj(path)\n",
    "    v_2 = torch.tensor(v_2).cuda()\n",
    "    f_2 = torch.tensor(f_2).cuda()\n",
    "    for i in range(3):\n",
    "        v_2[:, i] = v_2[:, i] - v_2[:, i].min()\n",
    "    ranges = v_2.max(dim=0)[0]\n",
    "    max_ind = torch.argmax(ranges)\n",
    "    max_len = ranges[max_ind]\n",
    "    scale_factor = 1/max_len\n",
    "    v_2 *= scale_factor\n",
    "    for i in range(3):\n",
    "        axis_range = v_2[:, i].max() - v_2[:, i].min()\n",
    "        v_2[:, i] += (1-axis_range)/2\n",
    "    triangles = v_2[f_2.flatten()].reshape(f_2.shape[0], f_2.shape[1], 3)\n",
    "    return v_2, f_2, triangles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "legal-spring",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mesh(name, res, obj_type):\n",
    "    path = f\"/mnt/nfs/home/saachij/data/libigl-tutorial-data/{name}\" \n",
    "    v, f, triangles = read_offset_and_normalize_custom(path, obj_type)\n",
    "    cpu_voxels, sparse_voxels = utils_3d.get_voxelization(triangles, res)\n",
    "    sparse_voxels = sparse_voxels.to_dense().unsqueeze(0)\n",
    "    return v, f, cpu_voxels, sparse_voxels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "elegant-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "RES = 0.05\n",
    "\n",
    "v_1, f_1, cpu_voxels_1, sparse_voxels_1 = get_mesh('armadillo.obj', RES, 'obj') \n",
    "v_2, f_2, cpu_voxels_2, sparse_voxels_2 = get_mesh('elephant.obj', RES, \"obj\")\n",
    "\n",
    "\n",
    "# viewer = mp.plot(cpu_voxels,  shading={\"point_size\": 0.03})\n",
    "# v_2, f_2, triangles = get_duck()\n",
    "# cpu_voxels_2, sparse_voxels_2 = utils_3d.get_voxelization(triangles, 0.075)\n",
    "# sparse_voxels_2 = sparse_voxels_2.to_dense().unsqueeze(0)\n",
    "# # viewer = mp.plot(cpu_voxels,  shading={\"point_size\": 0.03})\n",
    "SIZE = sparse_voxels_1.shape[-1]\n",
    "# print(sparse_voxels_1.shape, sparse_voxels_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-jesus",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = mp.plot(v_1.cpu().numpy(), f_1.cpu().numpy())\n",
    "draw_box(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "concrete-consumer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ba0b037fe44b9a9a41fe8eb88f8211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.4999999…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 17, 15] torch.Size([1, 20, 20, 20])\n",
      "[10, 15, 16] torch.Size([1, 20, 20, 20])\n",
      "[17, 17, 17] torch.Size([1, 20, 20, 20])\n",
      "[6, 1, 9] torch.Size([1, 20, 20, 20])\n",
      "[16, 1, 9] torch.Size([1, 20, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "# mp.subplot(v_2.cpu().numpy(), f_2.cpu().numpy())\n",
    "# p = mp.plot(cpu_voxels_1,  shading={\"point_size\": 0.03})\n",
    "p = visualize(sparse_voxels_1)\n",
    "mask = torch.zeros_like(sparse_voxels_1)\n",
    "v1_matched_points = [\n",
    "    [2, 17, 15], # left arm\n",
    "    [10, 15, 16], # nose\n",
    "    [17, 17, 17], # right arm\n",
    "    [6, 1, 9], # left toe\n",
    "    [16, 1, 9] # right toe\n",
    "]\n",
    "for point in v1_matched_points:\n",
    "    print(point, mask.shape)\n",
    "    mask[0][point[0], point[1], point[2]] = 1\n",
    "visualize(mask, p, \"blue\", point_size=0.1)\n",
    "draw_box(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "emotional-rating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in torch.stack(torch.where(sparse_voxels_1[0])).T:\n",
    "#     print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abroad-quilt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb52e0b675eb407da1b9c458bd6ed874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.4999999…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = mp.plot(v_2.cpu().numpy(), f_2.cpu().numpy())\n",
    "draw_box(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "spanish-champagne",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128b330b477f4d6a96746ea9db8f169b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.4999999…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 7, 10] torch.Size([1, 20, 20, 20])\n",
      "[10, 9, 14] torch.Size([1, 20, 20, 20])\n",
      "[16, 7, 10] torch.Size([1, 20, 20, 20])\n",
      "[8, 1, 9] torch.Size([1, 20, 20, 20])\n",
      "[12, 1, 9] torch.Size([1, 20, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "p = visualize(sparse_voxels_2)\n",
    "\n",
    "mask = torch.zeros_like(sparse_voxels_2)\n",
    "v2_matched_points = [\n",
    "    [4, 7, 10], # left arm\n",
    "    [10, 9, 14], # nose\n",
    "    [16, 7, 10], # right arm\n",
    "    [8, 1, 9], # left leg\n",
    "    [12, 1, 9] # right left\n",
    "]\n",
    "for point in v2_matched_points:\n",
    "    print(point, mask.shape)\n",
    "    mask[0][point[0], point[1], point[2]] = 1\n",
    "visualize(mask, p, \"blue\", point_size=0.1)\n",
    "draw_box(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bronze-death",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in torch.stack(torch.where(sparse_voxels_2[0])).T:\n",
    "#     print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dressed-increase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2, 17, 15],\n",
      "         [ 4,  7, 10]],\n",
      "\n",
      "        [[10, 15, 16],\n",
      "         [10,  9, 14]],\n",
      "\n",
      "        [[17, 17, 17],\n",
      "         [16,  7, 10]],\n",
      "\n",
      "        [[ 6,  1,  9],\n",
      "         [ 8,  1,  9]],\n",
      "\n",
      "        [[16,  1,  9],\n",
      "         [12,  1,  9]]])\n"
     ]
    }
   ],
   "source": [
    "assert len(v1_matched_points) == len(v2_matched_points)\n",
    "matched_points = []\n",
    "for i in range(len(v1_matched_points)):\n",
    "    matched_points.append([\n",
    "        v1_matched_points[i], v2_matched_points[i]\n",
    "    ])\n",
    "matched_points = torch.tensor(matched_points)\n",
    "print(matched_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "graphic-assumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_K_3D_custom(gamma, gamma_kp, alpha, R, matched_points, dims=[28, 28, 28]): #numr=28, numc=28):\n",
    "    print(\"gamma\", gamma, \"gamma_kp\", gamma_kp, \"alpha\", alpha, \"R\", R, \"matched_points\", matched_points)\n",
    "    num_mps = matched_points.shape[0]\n",
    "    numr, numc, numd = dims[:]\n",
    "\n",
    "    # these A and B are totally unrelated to the A and B in the names above \n",
    "    rA = torch.tensor(range(numr)).float()\n",
    "    rB = torch.tensor(range(numc)).float()\n",
    "    rC = torch.tensor(range(numd)).float()\n",
    "\n",
    "    # First image\n",
    "    rdists = (matched_points[:, 0, 0].view(num_mps, 1) - rA.view(1, numr))**2\n",
    "    cdists = (matched_points[:, 0, 1].view(num_mps, 1) - rB.view(1, numc))**2\n",
    "    ddists = (matched_points[:, 0, 2].view(num_mps, 1) - rC.view(1, numd))**2\n",
    "    matched_point_distsA = torch.sqrt(rdists.view(num_mps,numr,1,1) + cdists.view(num_mps,1,numc,1) + ddists.view(num_mps,1,1,numd))\n",
    "\n",
    "    # Second image\n",
    "    rdists = (matched_points[:, 1, 0].view(num_mps, 1) - rA.view(1, numr))**2\n",
    "    cdists = (matched_points[:, 1, 1].view(num_mps, 1) - rB.view(1, numc))**2\n",
    "    ddists = (matched_points[:, 1, 2].view(num_mps, 1) - rC.view(1, numd))**2\n",
    "    matched_point_distsB = torch.sqrt(rdists.view(num_mps,numr,1,1) + cdists.view(num_mps,1,numc,1) + ddists.view(num_mps,1,1,numd))\n",
    "\n",
    "    # threshold on matched_point_dists by R:\n",
    "    R = torch.tensor(R)\n",
    "    matched_point_distsA = torch.minimum(matched_point_distsA, R)\n",
    "    matched_point_distsB = torch.minimum(matched_point_distsB, R)\n",
    "    matched_point_distsA = matched_point_distsA.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "    matched_point_distsB = matched_point_distsB.unsqueeze(1).unsqueeze(1).unsqueeze(1)\n",
    "    C_keypoints = torch.mean((matched_point_distsA - matched_point_distsB)**2, dim=0)\n",
    "#     C_keypoints = torch.max((matched_point_distsA - matched_point_distsB)**2, dim=0)[0]\n",
    "    \n",
    "    #A = torch.tensor(range(numr))\n",
    "    rdiffs = (rA.view(numr, 1) - rA.view(1, numr))**2\n",
    "    #B = torch.tensor(range(numc))\n",
    "    cdiffs = (rB.view(numc, 1) - rB.view(1, numc))**2\n",
    "    ddiffs = (rC.view(numc, 1) - rC.view(1, numc))**2\n",
    "\n",
    "    C = rdiffs.view(numr,1,1,numr,1,1) + cdiffs.view(1,numc,1,1,numc,1) + ddiffs.view(1,1,numd,1,1,numd)\n",
    "    C = C.double()\n",
    "    C_keypoints = C_keypoints.double()\n",
    "    total_cost = (1-alpha)*C / (2* gamma**2) + alpha*C_keypoints / (2* gamma_kp**2)\n",
    "    K = torch.exp(-1 * total_cost)\n",
    "    norm_factor = K.flatten(3, 5).sum(dim=-1).unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "    return K/norm_factor, C, C_keypoints # note: given the two C's, can try diff alpha and gamma easily\n",
    "\n",
    "class CustomKeypoint(nn.Module):\n",
    "    def __init__(self, gamma, gamma_kp, alpha, R, matched_points, dims=[28, 28, 28]):\n",
    "        super().__init__()\n",
    "        K = make_K_3D_custom(gamma, gamma_kp, alpha, R, matched_points, dims)[0]\n",
    "        self.K = K.flatten(3, 5).permute(3, 0, 1, 2).flatten(1, 3).permute(1, 0).unsqueeze(0) # 1 x HWZ x HWZ\n",
    "        print(\"kernel\", self.K.min(), self.K.max())\n",
    "        \n",
    "    def forward_base(self, x, K):\n",
    "        # x is N, 1, H, W, Z\n",
    "        N, C, H, W, Z = x.shape\n",
    "        assert C == 1\n",
    "        x = x.flatten(2, 4).permute(0, 2, 1) # N, HWZ, 1\n",
    "        out = K @ x # N x HWZ x 1\n",
    "        out = out.permute(0, 2, 1) # N x 1 x HWZ\n",
    "        out = out.reshape(N, C, H, W, Z)\n",
    "        return out\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.forward_base(x, self.K)\n",
    "        \n",
    "    def forward_transpose(self, x):\n",
    "        return  self.forward_base(x, self.K.permute(0, 2, 1))\n",
    "\n",
    "\n",
    "class Gaussian3DBlur(torch.nn.Module):\n",
    "    def __init__(self, kernel, sigma):\n",
    "        super().__init__()\n",
    "        self.sigma = sigma\n",
    "        self.kernel = kernel\n",
    "        \n",
    "    def conv1d_blur(self, x):\n",
    "        # x is N, 1, H, W, Z\n",
    "        # we convolve over Z\n",
    "        kernel_size = self.kernel\n",
    "        sigma = self.sigma\n",
    "        k = tgm.image.gaussian.get_gaussian_kernel(kernel_size,sigma)\n",
    "        k = k.unsqueeze(0).unsqueeze(0)\n",
    "        orig_shape = x.shape\n",
    "        conv_inp = x.permute((4, 0, 1, 2, 3))\n",
    "        permuted_shape = conv_inp.shape\n",
    "        conv_inp = conv_inp.reshape(orig_shape[4], -1).permute(1,0).unsqueeze(1)\n",
    "        out = torch.nn.functional.conv1d(conv_inp, k, padding=(kernel_size - 1) // 2)\n",
    "        out = out.squeeze(1).permute(1,0).reshape(permuted_shape).permute(1, 2, 3, 4, 0)\n",
    "        return out\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x is N, 1, H, W, Z\n",
    "        x = self.conv1d_blur(x) # convolve over Z\n",
    "        \n",
    "        x = x.permute(0, 1, 2, 4, 3)\n",
    "        x = self.conv1d_blur(x) # convolve over W\n",
    "        x = x.permute(0, 1, 2, 4, 3)\n",
    "        \n",
    "        x = x.permute(0, 1, 3, 4, 2) # convolve over H\n",
    "        x = self.conv1d_blur(x)\n",
    "        x = x.permute(0, 1, 4, 2, 3)\n",
    "        return x\n",
    "    \n",
    "    def forward_transpose(self, x):\n",
    "        return self.forward(x)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "voluntary-jacob",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchgeometry as tgm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import fsolve\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class ConvolutionalWasserstein3D(nn.Module):\n",
    "    def __init__(self, nin, s, gamma, keypoint_args=None):\n",
    "        super().__init__()\n",
    "        self.a = torch.ones(1, nin, s, s, s).double() #/(s*s)\n",
    "        self.s = s\n",
    "        self.nin = nin\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        g_s = s\n",
    "        if s % 2 == 0:\n",
    "            g_s = s+1\n",
    "\n",
    "        if keypoint_args is not None:\n",
    "            self.kernel = CustomKeypoint(\n",
    "                gamma=gamma, gamma_kp=keypoint_args['gamma_kp'],\n",
    "                alpha=keypoint_args['alpha'], \n",
    "                R=keypoint_args['R'], \n",
    "                matched_points=keypoint_args['matched_points'], \n",
    "                dims=[s, s, s])\n",
    "        else:\n",
    "            self.kernel = Gaussian3DBlur(g_s, gamma)\n",
    "        \n",
    "    def sinkhorn(self, mu_0, mu_1, iters, return_plan=False):\n",
    "        mu_0 = mu_0.double()\n",
    "        mu_1 = mu_1.double()\n",
    "        w = torch.ones(*mu_0.shape).double()\n",
    "        v = torch.ones(*mu_0.shape).double()\n",
    "        for i in range(iters):\n",
    "            v = mu_0 /(self.kernel.forward(self.a*w))\n",
    "            w = mu_1 /(self.kernel.forward_transpose(self.a*v)) # changed this one to HT\n",
    "            print(i, v.flatten()[:4], w.flatten()[:4])\n",
    "        out = mu_0 * torch.log(v) + mu_1 * torch.log(w)\n",
    "        out = torch.flatten(out, 2, 4).unsqueeze(3) # N x C x H x W x Z\n",
    "        a = torch.flatten(self.a, 2, 4).unsqueeze(2)\n",
    "        out = (a @ out).squeeze(2).squeeze(2)\n",
    "        if return_plan:\n",
    "            return self.gamma * out, v, w\n",
    "        else:\n",
    "            return self.gamma * out\n",
    "        \n",
    "    def visualize_plan(self, pi_func, input_dist):\n",
    "        # Show one channel of the input distribution, and where it is sent\n",
    "        # input_dist does not have to be normalized to be a distribution\n",
    "        # e.g. pi_func = lambda x: v*self.H(w*x)\n",
    "        plt.figure(figsize=(15,7))\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(input_dist[0,0,...].cpu())\n",
    "        plt.title('Input distribution')\n",
    "        plt.colorbar()\n",
    "        \n",
    "        output = pi_func(input_dist)\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(output[0,0,...].cpu())\n",
    "        plt.title('Where it is sent')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        \n",
    "    def compute_entropy(self, p):\n",
    "        # p is (arbitrary batch dim) x nin x H x W x D\n",
    "        # returns vector of size (arbitrary batch dim) x nin, or nin, of the separate entropies\n",
    "        if len(p.shape) == 5:\n",
    "            a_reshape = self.a\n",
    "        elif len(p.shape) == 4:\n",
    "            a_reshape = self.a[0]\n",
    "        elif len(p.shape) == 3:\n",
    "            a_reshape = self.a[0,0]\n",
    "        z_locs = (p <= 0)\n",
    "        prod = a_reshape * p * torch.log(p) \n",
    "        prod[z_locs] = 0\n",
    "        out = -1*torch.sum(prod, dim=[-1, -2, -3])\n",
    "        return out\n",
    "    \n",
    "    def entropic_sharpening(self, mu, H0):\n",
    "        # mu is C x H x W\n",
    "        # H0 is C \n",
    "        C, H, W, D = mu.shape\n",
    "        \n",
    "        # Doing this unbatched in C for now\n",
    "        betas = torch.ones(C)\n",
    "        for i in range(C):\n",
    "            mu_ent = self.compute_entropy(mu[i,...])\n",
    "            if mu_ent + torch.sum(mu[i, ...]*self.a[0, i, ...]) > H0[i] + 1:\n",
    "                def f(beta):\n",
    "                    mu_to_beta = torch.pow(mu[i,...], torch.tensor(beta))\n",
    "                    return torch.sum(self.a[0, i, ...]*mu_to_beta) + self.compute_entropy(mu_to_beta) - (1 + H0[i])\n",
    "                xx = fsolve(f, x0=[1]) # Does not include positivity constraint, not ideal.\n",
    "                if xx[0] >= 0:\n",
    "                    betas[i] = xx[0]\n",
    "        sharpened = torch.pow(mu, betas.view(C, 1, 1, 1))\n",
    "        return sharpened\n",
    "    \n",
    "    def wass_barycenter(self, mu_s, alphas, iters, entropic_args=None): # NOT BATCHED\n",
    "        # mu_s is K x C x H x W\n",
    "        # alphas is K\n",
    "        # M is the number of mu_s\n",
    "        # entropic_args['factor'] changes the level of sharpening by scaling the maximum entropy\n",
    "        # otherwise, setting entropic_args as None means to not do entropic sharpening\n",
    "        K, C, H, W, Z = mu_s.shape\n",
    "        \n",
    "        if entropic_args is not None:\n",
    "            H0 = torch.max(self.compute_entropy(mu_s), dim=0).values\n",
    "            H0 = entropic_args['factor']*H0 \n",
    "        \n",
    "        v = torch.ones_like(mu_s)\n",
    "        w = torch.ones_like(mu_s)\n",
    "        for j in range(iters):\n",
    "            print(j)\n",
    "            w = mu_s / self.kernel.forward_transpose(self.a * v) # changed to HT\n",
    "            d = v * self.kernel.forward(self.a * w)\n",
    "            mu = torch.ones(C, H, W, Z)\n",
    "            for i in range(K):\n",
    "                mu = mu * torch.pow(d[i], alphas[i])\n",
    "            if entropic_args is not None:\n",
    "                mu = self.entropic_sharpening(mu, H0)\n",
    "            v = (v * mu.unsqueeze(0))/d\n",
    "        return mu\n",
    "    \n",
    "    def wass_barycenter_obj(self, mu, mu_s, alphas):\n",
    "        # mu_s is K x C x H x W\n",
    "        # alphas is K\n",
    "        # mu is C x H x W\n",
    "        K, C, H, W = mu_s.shape\n",
    "        \n",
    "        mu_repped = mu.repeat(K, 1, 1, 1)\n",
    "        W2dists = self.sinkhorn(mu_repped, mu_s, iters=10) # K x C\n",
    "        objvals = W2dists * alphas.view(-1,1).repeat_interleave(repeats=C, dim=-1) # K x C\n",
    "        return torch.sum(objvals, 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "interior-examination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2, 17, 15],\n",
       "         [ 4,  7, 10]],\n",
       "\n",
       "        [[10, 15, 16],\n",
       "         [10,  9, 14]],\n",
       "\n",
       "        [[17, 17, 17],\n",
       "         [16,  7, 10]],\n",
       "\n",
       "        [[ 6,  1,  9],\n",
       "         [ 8,  1,  9]],\n",
       "\n",
       "        [[16,  1,  9],\n",
       "         [12,  1,  9]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "surprising-feedback",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma 0.1 gamma_kp 0.5 alpha 0 R 11 matched_points tensor([[[ 2., 17., 15.],\n",
      "         [ 4.,  7., 10.]],\n",
      "\n",
      "        [[10., 15., 16.],\n",
      "         [10.,  9., 14.]],\n",
      "\n",
      "        [[17., 17., 17.],\n",
      "         [16.,  7., 10.]],\n",
      "\n",
      "        [[ 6.,  1.,  9.],\n",
      "         [ 8.,  1.,  9.]],\n",
      "\n",
      "        [[16.,  1.,  9.],\n",
      "         [12.,  1.,  9.]]], dtype=torch.float64)\n",
      "kernel tensor(0., dtype=torch.float64) tensor(1., dtype=torch.float64)\n",
      "0 tensor([1.6155e-05, 1.6155e-05, 1.6155e-05, 1.6155e-05], dtype=torch.float64) tensor([0.7737, 0.7737, 0.7737, 0.7737], dtype=torch.float64)\n",
      "1 tensor([2.0879e-05, 2.0879e-05, 2.0879e-05, 2.0879e-05], dtype=torch.float64) tensor([0.5987, 0.5987, 0.5987, 0.5987], dtype=torch.float64)\n",
      "2 tensor([2.6984e-05, 2.6984e-05, 2.6984e-05, 2.6984e-05], dtype=torch.float64) tensor([0.4632, 0.4632, 0.4632, 0.4632], dtype=torch.float64)\n",
      "3 tensor([3.4874e-05, 3.4874e-05, 3.4874e-05, 3.4874e-05], dtype=torch.float64) tensor([0.3584, 0.3584, 0.3584, 0.3584], dtype=torch.float64)\n",
      "4 tensor([4.5072e-05, 4.5072e-05, 4.5072e-05, 4.5072e-05], dtype=torch.float64) tensor([0.2773, 0.2773, 0.2773, 0.2773], dtype=torch.float64)\n",
      "5 tensor([5.8251e-05, 5.8251e-05, 5.8251e-05, 5.8251e-05], dtype=torch.float64) tensor([0.2146, 0.2146, 0.2146, 0.2146], dtype=torch.float64)\n",
      "6 tensor([7.5284e-05, 7.5284e-05, 7.5284e-05, 7.5284e-05], dtype=torch.float64) tensor([0.1660, 0.1660, 0.1660, 0.1660], dtype=torch.float64)\n",
      "7 tensor([9.7298e-05, 9.7298e-05, 9.7298e-05, 9.7298e-05], dtype=torch.float64) tensor([0.1285, 0.1285, 0.1285, 0.1285], dtype=torch.float64)\n",
      "8 tensor([0.0001, 0.0001, 0.0001, 0.0001], dtype=torch.float64) tensor([0.0994, 0.0994, 0.0994, 0.0994], dtype=torch.float64)\n",
      "9 tensor([0.0002, 0.0002, 0.0002, 0.0002], dtype=torch.float64) tensor([0.0769, 0.0769, 0.0769, 0.0769], dtype=torch.float64)\n",
      "10 tensor([0.0002, 0.0002, 0.0002, 0.0002], dtype=torch.float64) tensor([0.0595, 0.0595, 0.0595, 0.0595], dtype=torch.float64)\n",
      "11 tensor([0.0003, 0.0003, 0.0003, 0.0003], dtype=torch.float64) tensor([0.0460, 0.0460, 0.0460, 0.0460], dtype=torch.float64)\n",
      "12 tensor([0.0004, 0.0004, 0.0004, 0.0004], dtype=torch.float64) tensor([0.0356, 0.0356, 0.0356, 0.0356], dtype=torch.float64)\n",
      "13 tensor([0.0005, 0.0005, 0.0005, 0.0005], dtype=torch.float64) tensor([0.0276, 0.0276, 0.0276, 0.0276], dtype=torch.float64)\n",
      "14 tensor([0.0006, 0.0006, 0.0006, 0.0006], dtype=torch.float64) tensor([0.0213, 0.0213, 0.0213, 0.0213], dtype=torch.float64)\n",
      "15 tensor([0.0008, 0.0008, 0.0008, 0.0008], dtype=torch.float64) tensor([0.0165, 0.0165, 0.0165, 0.0165], dtype=torch.float64)\n",
      "16 tensor([0.0010, 0.0010, 0.0010, 0.0010], dtype=torch.float64) tensor([0.0128, 0.0128, 0.0128, 0.0128], dtype=torch.float64)\n",
      "17 tensor([0.0013, 0.0013, 0.0013, 0.0013], dtype=torch.float64) tensor([0.0099, 0.0099, 0.0099, 0.0099], dtype=torch.float64)\n",
      "18 tensor([0.0016, 0.0016, 0.0016, 0.0016], dtype=torch.float64) tensor([0.0076, 0.0076, 0.0076, 0.0076], dtype=torch.float64)\n",
      "19 tensor([0.0021, 0.0021, 0.0021, 0.0021], dtype=torch.float64) tensor([0.0059, 0.0059, 0.0059, 0.0059], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "keypoint_args = { #0.5, 10\n",
    "    'alpha': 0,\n",
    "    'gamma_kp': 0.5,\n",
    "    'R': 11,\n",
    "    'matched_points': matched_points.double(),\n",
    "    'gamma': 0.1, # unused in actual class\n",
    "    'iters': 20 # unused in actual class\n",
    "}\n",
    "\n",
    "# keypoint_args = { #0.5, 10\n",
    "#     'alpha': 1,\n",
    "#     'gamma_kp': 0.5,\n",
    "#     'R': 10,\n",
    "#     'matched_points': matched_points.double(),\n",
    "#     'gamma': 0.1, # unused in actual class\n",
    "#     'iters': 20 # unused in actual class\n",
    "# }\n",
    "\n",
    "\n",
    "cwd = ConvolutionalWasserstein3D(1, SIZE, keypoint_args['gamma'], keypoint_args)\n",
    "inp_1 = sparse_voxels_1 + 1e-2\n",
    "inp_1 = inp_1/inp_1.sum()\n",
    "inp_2 = sparse_voxels_2 + 1e-2\n",
    "inp_2 = inp_2/inp_2.sum()\n",
    "out, w, v = cwd.sinkhorn(inp_1, inp_2, keypoint_args['iters'], return_plan=True)\n",
    "plan = lambda x: v*cwd.kernel.forward(w*x)\n",
    "planT = lambda x: w*cwd.kernel.forward_transpose(v*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "valid-austria",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7a131e4dc249bb9b43dbacdfc03b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.7999999…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<meshplot.Viewer.Viewer at 0x7f1037801310>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1_coords = torch.stack(torch.where(sparse_voxels_1[0])).T\n",
    "mask = torch.zeros_like(w)\n",
    "# kp = torch.tensor([17, 17, 17])# , radius 8\n",
    "# kp = torch.tensor([6, 1, 9]) # radius 7\n",
    "kp =  torch.tensor([30, 17, 17])\n",
    "for i in range(v1_coords.shape[0]):\n",
    "    coords = v1_coords[i]\n",
    "    if torch.linalg.norm(coords.float() - kp.float()) >=18:\n",
    "        continue\n",
    "    mask[:, :, coords[0], coords[1], coords[2]] = 1\n",
    "visualize(mask[0])\n",
    "\n",
    "# v1_coords = torch.stack(torch.where(sparse_voxels_1[0])).T\n",
    "# mask = torch.zeros_like(w)\n",
    "# kp = torch.tensor([19, 17, 17])\n",
    "\n",
    "# for i in range(v1_coords.shape[0]):\n",
    "#     coords = v1_coords[i]\n",
    "#     if torch.linalg.norm(coords.float() - kp.float()) >=7:\n",
    "#         continue\n",
    "#     mask[:, :, coords[0], coords[1], coords[2]] = 1\n",
    "# visualize(mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "opposed-assumption",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38389cbd9823431884b2942f6e57e00b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.4999999…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = planT(mask)[0, 0]\n",
    "dist = dist/dist.sum()\n",
    "thresh = 2e-3\n",
    "cube = torch.ones_like(dist)\n",
    "voxels = torch.stack(torch.where(cube)).T.numpy()\n",
    "# p = mp.plot(voxels/SIZE, c='black', shading={\"point_size\": 0.02})\n",
    "p = mp.plot(cpu_voxels_2, c='red', shading={\"point_size\": 0.04})\n",
    "# p.add_points(cpu_voxels_1, c='blue', shading={\"point_size\": 0.04})\n",
    "mask_voxels = torch.stack(torch.where(mask[0, 0])).T.numpy()\n",
    "p.add_points(mask_voxels/SIZE, c=\"gray\", shading={\"point_shape\": \"square\", \"point_size\": 0.04})\n",
    "p.add_points((voxels/SIZE)[dist.flatten() > thresh], c=dist.flatten()[dist.flatten() > thresh].numpy(), shading={\"point_shape\": \"circle\", \"point_size\": 0.07})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "complicated-imaging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Double but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f8113b57682d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minp_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp_2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0minp_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mentropic_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'factor'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwass_barycenter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minp_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropic_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-a89fb32a3ae6>\u001b[0m in \u001b[0;36mwass_barycenter\u001b[0;34m(self, mu_s, alphas, iters, entropic_args)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmu_s\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_transpose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# changed to HT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-04a9951ba706>\u001b[0m in \u001b[0;36mforward_transpose\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_transpose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-04a9951ba706>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# x is N, 1, H, W, Z\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1d_blur\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# convolve over Z\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-04a9951ba706>\u001b[0m in \u001b[0;36mconv1d_blur\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mpermuted_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_inp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mconv_inp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_inp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermuted_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Double but found Float"
     ]
    }
   ],
   "source": [
    "cwd = ConvolutionalWasserstein3D(1, SIZE, 0.1)\n",
    "inp_1 = sparse_voxels_1 + 1e-2\n",
    "inp_1 = inp_1/inp_1.sum()\n",
    "inp_2 = sparse_voxels_2 + 1e-2\n",
    "inp_2 = inp_2/inp_2.sum()\n",
    "entropic_args = {'factor': 0.3}\n",
    "out = cwd.wass_barycenter(torch.stack([inp_1, inp_2]),  torch.tensor([0.5, 0.5]), 10, entropic_args=None)\n",
    "out[out < 1e-4] = 0\n",
    "p = visualize(out)\n",
    "draw_box(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-albuquerque",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
